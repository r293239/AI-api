import numpy as np
import json
import pickle
from typing import List, Tuple, Optional, Dict, Any, Callable, Union
import random
from abc import ABC, abstractmethod
import time
from collections import deque
import math

class PrioritizedMemory:
    """Prioritized Experience Replay for more efficient learning"""
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6, beta: float = 0.4):
        self.capacity = capacity
        self.alpha = alpha  # Prioritization strength
        self.beta = beta    # Importance sampling correction
        self.beta_increment = 0.001
        self.epsilon = 1e-6  # Small constant to avoid zero priorities
        
        self.memory = []
        self.priorities = np.zeros((capacity,), dtype=np.float32)
        self.position = 0
        self.max_priority = 1.0
    
    def push(self, state, action, reward, next_state, done):
        """Store experience with maximum priority"""
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        
        self.memory[self.position] = (state, action, reward, next_state, done)
        self.priorities[self.position] = self.max_priority
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int):
        """Sample experiences based on priorities"""
        if len(self.memory) < batch_size:
            indices = np.arange(len(self.memory))
        else:
            # Calculate sampling probabilities
            priorities = self.priorities[:len(self.memory)]
            probs = priorities ** self.alpha
            probs /= probs.sum()
            
            # Sample indices based on priorities
            indices = np.random.choice(len(self.memory), batch_size, p=probs, replace=False)
        
        # Calculate importance sampling weights
        weights = (len(self.memory) * probs[indices]) ** (-self.beta)
        weights /= weights.max()  # Normalize
        
        experiences = [self.memory[idx] for idx in indices]
        
        # Increment beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        return experiences, indices, weights
    
    def update_priorities(self, indices, td_errors):
        """Update priorities based on TD errors"""
        for idx, td_error in zip(indices, td_errors):
            priority = (abs(td_error) + self.epsilon) ** self.alpha
            self.priorities[idx] = priority
            self.max_priority = max(self.max_priority, priority)
    
    def __len__(self):
        return len(self.memory)

class AdvancedNeuralNetwork:
    """Enhanced neural network with better architecture and optimization"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, 
                 learning_rate: float = 0.001, dropout_rate: float = 0.1):
        self.learning_rate = learning_rate
        self.dropout_rate = dropout_rate
        self.layers = []
        self.momentum = 0.9  # For momentum optimization
        self.beta1, self.beta2 = 0.9, 0.999  # For Adam optimizer
        self.epsilon = 1e-8
        self.t = 0  # Time step for Adam
        
        # Build network with better initialization
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            # Xavier/He initialization for better convergence
            fan_in, fan_out = sizes[i], sizes[i+1]
            limit = np.sqrt(6.0 / (fan_in + fan_out))
            
            layer = {
                'weights': np.random.uniform(-limit, limit, (sizes[i], sizes[i+1])),
                'biases': np.zeros((1, sizes[i+1])),
                'activations': None,
                'z_values': None,
                # Adam optimizer parameters
                'm_weights': np.zeros((sizes[i], sizes[i+1])),
                'v_weights': np.zeros((sizes[i], sizes[i+1])),
                'm_biases': np.zeros((1, sizes[i+1])),
                'v_biases': np.zeros((1, sizes[i+1]))
            }
            self.layers.append(layer)
    
    def leaky_relu(self, x, alpha=0.01):
        """Leaky ReLU activation (better than ReLU for avoiding dead neurons)"""
        return np.where(x > 0, x, alpha * x)
    
    def leaky_relu_derivative(self, x, alpha=0.01):
        return np.where(x > 0, 1, alpha)
    
    def forward(self, x, training=True):
        """Forward pass with optional dropout"""
        activation = x.reshape(1, -1) if x.ndim == 1 else x
        
        for i, layer in enumerate(self.layers):
            z = np.dot(activation, layer['weights']) + layer['biases']
            layer['z_values'] = z
            
            if i < len(self.layers) - 1:  # Hidden layers
                activation = self.leaky_relu(z)
                # Apply dropout during training
                if training and self.dropout_rate > 0:
                    dropout_mask = np.random.binomial(1, 1-self.dropout_rate, activation.shape) / (1-self.dropout_rate)
                    activation *= dropout_mask
                    layer['dropout_mask'] = dropout_mask
            else:  # Output layer (linear)
                activation = z
                
            layer['activations'] = activation
        
        return activation.flatten() if activation.shape[0] == 1 else activation
    
    def backward(self, x, target, importance_weight=1.0):
        """Enhanced backpropagation with Adam optimizer"""
        # Forward pass first
        output = self.forward(x, training=True)
        
        # Calculate loss gradient (MSE) with importance weighting
        output_error = (output.reshape(1, -1) - target.reshape(1, -1)) * importance_weight
        
        self.t += 1  # Increment time step for Adam
        
        # Backward pass
        for i in reversed(range(len(self.layers))):
            layer = self.layers[i]
            
            if i == len(self.layers) - 1:  # Output layer
                delta = output_error
            else:  # Hidden layers
                delta = np.dot(delta, self.layers[i+1]['weights'].T) * self.leaky_relu_derivative(layer['z_values'])
                # Apply dropout mask if it exists
                if hasattr(layer, 'dropout_mask'):
                    delta *= layer['dropout_mask']
            
            # Get input to this layer
            if i == 0:
                layer_input = x.reshape(1, -1)
            else:
                layer_input = self.layers[i-1]['activations']
            
            # Calculate gradients
            weight_grad = np.dot(layer_input.T, delta)
            bias_grad = np.sum(delta, axis=0, keepdims=True)
            
            # Adam optimizer updates
            layer['m_weights'] = self.beta1 * layer['m_weights'] + (1 - self.beta1) * weight_grad
            layer['v_weights'] = self.beta2 * layer['v_weights'] + (1 - self.beta2) * (weight_grad ** 2)
            layer['m_biases'] = self.beta1 * layer['m_biases'] + (1 - self.beta1) * bias_grad
            layer['v_biases'] = self.beta2 * layer['v_biases'] + (1 - self.beta2) * (bias_grad ** 2)
            
            # Bias correction
            m_weights_corrected = layer['m_weights'] / (1 - self.beta1 ** self.t)
            v_weights_corrected = layer['v_weights'] / (1 - self.beta2 ** self.t)
            m_biases_corrected = layer['m_biases'] / (1 - self.beta1 ** self.t)
            v_biases_corrected = layer['v_biases'] / (1 - self.beta2 ** self.t)
            
            # Update parameters
            layer['weights'] -= self.learning_rate * m_weights_corrected / (np.sqrt(v_weights_corrected) + self.epsilon)
            layer['biases'] -= self.learning_rate * m_biases_corrected / (np.sqrt(v_biases_corrected) + self.epsilon)
    
    def copy(self):
        """Create a deep copy of the network"""
        new_net = AdvancedNeuralNetwork(1, [], 1)  # Dummy initialization
        new_net.layers = []
        new_net.learning_rate = self.learning_rate
        new_net.dropout_rate = self.dropout_rate
        
        for layer in self.layers:
            new_layer = {key: value.copy() if isinstance(value, np.ndarray) else value 
                        for key, value in layer.items()}
            new_net.layers.append(new_layer)
        return new_net

class CuriosityModule:
    """Intrinsic motivation through curiosity-driven learning"""
    
    def __init__(self, state_size: int, action_size: int, learning_rate: float = 0.001):
        # Forward model predicts next state given current state and action
        self.forward_model = AdvancedNeuralNetwork(
            state_size + action_size, [64, 32], state_size, learning_rate
        )
        
        # Inverse model predicts action given current and next state
        self.inverse_model = AdvancedNeuralNetwork(
            state_size * 2, [64, 32], action_size, learning_rate
        )
        
        self.curiosity_weight = 0.1  # Weight for intrinsic reward
        
    def compute_intrinsic_reward(self, state, action, next_state):
        """Compute curiosity-based intrinsic reward"""
        # One-hot encode action
        action_onehot = np.zeros(self.inverse_model.layers[-1]['weights'].shape[1])
        if 0 <= action < len(action_onehot):
            action_onehot[action] = 1
        
        # Predict next state
        state_action = np.concatenate([state, action_onehot])
        predicted_next_state = self.forward_model.forward(state_action, training=False)
        
        # Curiosity is the prediction error
        prediction_error = np.mean((predicted_next_state - next_state) ** 2)
        intrinsic_reward = self.curiosity_weight * prediction_error
        
        return intrinsic_reward
    
    def update(self, state, action, next_state):
        """Update curiosity models"""
        # One-hot encode action
        action_onehot = np.zeros(self.inverse_model.layers[-1]['weights'].shape[1])
        if 0 <= action < len(action_onehot):
            action_onehot[action] = 1
        
        # Train forward model
        state_action = np.concatenate([state, action_onehot])
        self.forward_model.backward(state_action, next_state)
        
        # Train inverse model
        state_next_state = np.concatenate([state, next_state])
        self.inverse_model.backward(state_next_state, action_onehot)

class MetaLearningAgent:
    """Advanced agent with meta-learning capabilities"""
    
    def __init__(self, state_size: int, action_size: int, learning_rate: float = 0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = PrioritizedMemory(capacity=5000)
        
        # Enhanced hyperparameters
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.9999  # Slower decay for better exploration
        self.gamma = 0.99  # Higher discount for longer-term thinking
        self.batch_size = 64  # Larger batch size
        
        # Double DQN networks
        self.q_network = AdvancedNeuralNetwork(state_size, [128, 128, 64], action_size, learning_rate)
        self.target_network = self.q_network.copy()
        self.update_target_frequency = 200
        self.update_counter = 0
        
        # Curiosity module for intrinsic motivation
        self.curiosity_module = CuriosityModule(state_size, action_size, learning_rate)
        
        # Multi-step learning
        self.n_step = 3
        self.n_step_buffer = deque(maxlen=self.n_step)
        
        # Adaptive exploration
        self.exploration_bonus = 0.1
        self.visit_counts = {}
        
        # Learning statistics
        self.episode_rewards = []
        self.episode_steps = []
        self.td_errors = []
        self.intrinsic_rewards = []
        
    def get_state_key(self, state):
        """Create hashable key for state visitation counting"""
        return tuple(np.round(state, 2))  # Round for discretization
    
    def compute_exploration_bonus(self, state):
        """Compute exploration bonus based on state visitation"""
        state_key = self.get_state_key(state)
        visit_count = self.visit_counts.get(state_key, 0)
        return self.exploration_bonus / math.sqrt(visit_count + 1)
    
    def act(self, state, training=True):
        """Enhanced action selection with UCB exploration"""
        if training:
            # Update visit count
            state_key = self.get_state_key(state)
            self.visit_counts[state_key] = self.visit_counts.get(state_key, 0) + 1
            
            # Epsilon-greedy with exploration bonus
            if np.random.random() <= self.epsilon:
                return np.random.choice(self.action_size)
            
            # Add exploration bonus to Q-values
            q_values = self.q_network.forward(state, training=False)
            exploration_bonus = self.compute_exploration_bonus(state)
            q_values += exploration_bonus
            
            return np.argmax(q_values)
        else:
            q_values = self.q_network.forward(state, training=False)
            return np.argmax(q_values)
    
    def remember(self, state, action, reward, next_state, done):
        """Enhanced memory storage with n-step returns"""
        # Add to n-step buffer
        self.n_step_buffer.append((state, action, reward, next_state, done))
        
        if len(self.n_step_buffer) == self.n_step:
            # Calculate n-step return
            n_step_return = 0
            gamma_n = 1
            
            for i, (_, _, r, _, d) in enumerate(self.n_step_buffer):
                n_step_return += gamma_n * r
                gamma_n *= self.gamma
                if d:
                    break
            
            # Store n-step experience
            first_state, first_action, _, _, _ = self.n_step_buffer[0]
            last_state, _, _, next_state, done = self.n_step_buffer[-1]
            
            self.memory.push(first_state, first_action, n_step_return, next_state, done)
    
    def replay(self):
        """Enhanced training with prioritized experience replay"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample from prioritized memory
        experiences, indices, importance_weights = self.memory.sample(self.batch_size)
        td_errors = []
        
        for i, (state, action, reward, next_state, done) in enumerate(experiences):
            # Double DQN target calculation
            if not done:
                # Use main network to select action
                next_actions = np.argmax(self.q_network.forward(next_state, training=False))
                # Use target network to evaluate action
                next_q_values = self.target_network.forward(next_state, training=False)
                target = reward + self.gamma * next_q_values[next_actions]
            else:
                target = reward
            
            # Add intrinsic reward from curiosity
            if not done:
                intrinsic_reward = self.curiosity_module.compute_intrinsic_reward(state, action, next_state)
                target += intrinsic_reward
                self.intrinsic_rewards.append(intrinsic_reward)
            
            # Calculate TD error for priority update
            current_q_values = self.q_network.forward(state, training=False)
            td_error = abs(target - current_q_values[action])
            td_errors.append(td_error)
            
            # Train network with importance sampling
            target_q_values = current_q_values.copy()
            target_q_values[action] = target
            
            self.q_network.backward(state, target_q_values, importance_weights[i])
            
            # Update curiosity module
            if not done:
                self.curiosity_module.update(state, action, next_state)
        
        # Update priorities
        self.memory.update_priorities(indices, td_errors)
        self.td_errors.extend(td_errors)
        
        # Adaptive epsilon decay based on performance
        if len(self.episode_rewards) > 10:
            recent_performance = np.mean(self.episode_rewards[-10:])
            if len(self.episode_rewards) > 20:
                older_performance = np.mean(self.episode_rewards[-20:-10])
                if recent_performance > older_performance:
                    self.epsilon_decay = max(0.999, self.epsilon_decay)  # Decay faster if improving
                else:
                    self.epsilon_decay = min(0.9999, self.epsilon_decay + 0.0001)  # Decay slower if not improving
        
        # Decay exploration
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # Update target network
        self.update_counter += 1
        if self.update_counter % self.update_target_frequency == 0:
            self.target_network = self.q_network.copy()
    
    def train(self, environment, episodes: int = 1000, verbose: bool = True):
        """Enhanced training with curriculum learning"""
        self.episode_rewards = []
        self.episode_steps = []
        
        # Curriculum learning: start with easier episodes
        curriculum_episodes = min(100, episodes // 4)
        
        for episode in range(episodes):
            state = environment.reset()
            
            # Curriculum learning: reduce episode length initially
            if episode < curriculum_episodes:
                max_steps = min(50, 200 * (episode + 1) // curriculum_episodes)
            else:
                max_steps = 500
            
            total_reward = 0
            steps = 0
            
            while steps < max_steps:
                action = self.act(state)
                next_state, reward, done, _ = environment.step(action)
                
                self.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            self.episode_rewards.append(total_reward)
            self.episode_steps.append(steps)
            
            # Train more frequently for faster learning
            if episode % 4 == 0:
                for _ in range(2):  # Multiple training steps per episode
                    self.replay()
            
            if verbose and episode % 50 == 0:
                avg_reward = np.mean(self.episode_rewards[-50:])
                avg_steps = np.mean(self.episode_steps[-50:])
                avg_td_error = np.mean(self.td_errors[-100:]) if self.td_errors else 0
                avg_intrinsic = np.mean(self.intrinsic_rewards[-100:]) if self.intrinsic_rewards else 0
                
                print(f"Episode {episode}")
                print(f"  Avg Reward: {avg_reward:.2f}")
                print(f"  Avg Steps: {avg_steps:.2f}")
                print(f"  Epsilon: {self.epsilon:.4f}")
                print(f"  TD Error: {avg_td_error:.4f}")
                print(f"  Intrinsic Reward: {avg_intrinsic:.4f}")
                print(f"  States Explored: {len(self.visit_counts)}")
                print()
    
    def test(self, environment, episodes: int = 5):
        """Test the trained agent with detailed analysis"""
        print("\n--- Testing Enhanced Agent ---")
        test_rewards = []
        test_steps = []
        
        for episode in range(episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            print(f"\nTest Episode {episode + 1}:")
            if hasattr(environment, 'render'):
                environment.render()
            
            while True:
                action = self.act(state, training=False)
                q_values = self.q_network.forward(state, training=False)
                
                state, reward, done, _ = environment.step(action)
                total_reward += reward
                steps += 1
                
                print(f"Step {steps}: Action {action} (Q-values: {q_values}), Reward {reward}")
                if hasattr(environment, 'render'):
                    environment.render()
                
                if done or steps > 100:
                    break
                
                time.sleep(0.3)
            
            test_rewards.append(total_reward)
            test_steps.append(steps)
            print(f"Episode finished: {total_reward} reward in {steps} steps")
        
        avg_reward = np.mean(test_rewards)
        avg_steps = np.mean(test_steps)
        
        print(f"\nTest Results:")
        print(f"  Average Reward: {avg_reward:.2f}")
        print(f"  Average Steps: {avg_steps:.2f}")
        print(f"  Success Rate: {sum(1 for r in test_rewards if r > 0) / len(test_rewards) * 100:.1f}%")
        
        return avg_reward

# Enhanced Environments with more complexity

class AdvancedGridWorld:
    """More complex grid world with multiple objectives and obstacles"""
    
    def __init__(self, width=6, height=6):
        self.width = width
        self.height = height
        self.reset()
        
        # Enhanced reward structure
        self.goal_reward = 100
        self.step_penalty = -0.1  # Smaller penalty for more exploration
        self.wall_penalty = -5
        self.treasure_reward = 50
        self.trap_penalty = -20
        
    def reset(self):
        self.agent_pos = [0, 0]
        self.goal_pos = [self.width-1, self.height-1]
        
        # More complex environment
        self.walls = [(2, 2), (2, 3), (3, 2), (1, 4), (4, 1)]
        self.treasures = [(1, 1), (3, 4)]  # Optional rewards
        self.traps = [(2, 1), (4, 3)]      # Penalties
        self.collected_treasures = set()
        
        return self.get_state()
    
    def get_state(self):
        """Enhanced state representation"""
        # Larger state space with more information
        state = np.zeros(self.width * self.height * 4 + 8)  # Multiple channels + features
        
        # Agent position channel
        agent_idx = self.agent_pos[1] * self.width + self.agent_pos[0]
        state[agent_idx] = 1
        
        # Goal position channel
        goal_idx = self.goal_pos[1] * self.width + self.goal_pos[0]
        state[self.width * self.height + goal_idx] = 1
        
        # Treasure channel
        for treasure in self.treasures:
            if treasure not in self.collected_treasures:
                treasure_idx = treasure[1] * self.width + treasure[0]
                state[2 * self.width * self.height + treasure_idx] = 1
        
        # Wall channel
        for wall in self.walls:
            wall_idx = wall[1] * self.width + wall[0]
            state[3 * self.width * self.height + wall_idx] = 1
        
        # Additional features
        features_start = 4 * self.width * self.height
        state[features_start] = self.agent_pos[0] / self.width      # Normalized x
        state[features_start + 1] = self.agent_pos[1] / self.height  # Normalized y
        state[features_start + 2] = self.goal_pos[0] / self.width    # Goal x
        state[features_start + 3] = self.goal_pos[1] / self.height   # Goal y
        
        # Distance to goal
        goal_distance = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])
        state[features_start + 4] = goal_distance / (self.width + self.height)
        
        # Number of treasures collected
        state[features_start + 5] = len(self.collected_treasures) / len(self.treasures)
        
        # Distance to nearest treasure
        if self.treasures:
            min_treasure_dist = min(
                abs(self.agent_pos[0] - t[0]) + abs(self.agent_pos[1] - t[1])
                for t in self.treasures if t not in self.collected_treasures
            )
            state[features_start + 6] = min_treasure_dist / (self.width + self.height)
        
        # Bias term
        state[features_start + 7] = 1.0
        
        return state
    
    def step(self, action):
        """Enhanced step function with more complex dynamics"""
        old_pos = self.agent_pos.copy()
        
        # Apply action with some stochasticity
        if np.random.random() < 0.05:  # 5% chance of random action
            action = np.random.choice(4)
        
        if action == 0:  # Up
            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)
        elif action == 1:  # Right
            self.agent_pos[0] = min(self.width-1, self.agent_pos[0] + 1)
        elif action == 2:  # Down
            self.agent_pos[1] = min(self.height-1, self.agent_pos[1] + 1)
        elif action == 3:  # Left
            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)
        
        reward = self.step_penalty
        done = False
        
        # Check interactions
        if tuple(self.agent_pos) in self.walls:
            self.agent_pos = old_pos  # Bounce back
            reward = self.wall_penalty
        elif self.agent_pos == self.goal_pos:
            reward = self.goal_reward
            # Bonus for collecting treasures
            reward += len(self.collected_treasures) * 20
            done = True
        elif tuple(self.agent_pos) in self.treasures and tuple(self.agent_pos) not in self.collected_treasures:
            self.collected_treasures.add(tuple(self.agent_pos))
            reward += self.treasure_reward
        elif tuple(self.agent_pos) in self.traps:
            reward += self.trap_penalty
        
        return self.get_state(), reward, done, {'treasures_collected': len(self.collected_treasures)}
    
    def get_state_size(self):
        return self.width * self.height * 4 + 8
    
    def get_action_size(self):
        return 4
    
    def render(self):
        """Enhanced visualization"""
        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]
        
        # Add elements
        for wall in self.walls:
            grid[wall[1]][wall[0]] = '‚ñà'
        
        for treasure in self.treasures:
            if treasure not in self.collected_treasures:
                grid[treasure[1]][treasure[0]] = '$'
        
        for trap in self.traps:
            grid[trap[1]][trap[0]] = 'X'
        
        grid[self.goal_pos[1]][self.goal_pos[0]] = 'G'
        grid[self.agent_pos[1]][self.agent_pos[0]] = 'A'
        
        print(f"Treasures collected: {len(self.collected_treasures)}/{len(self.treasures)}")
        for row in grid:
            print(' '.join(row))
        print()

class SmartAI:
    """Enhanced AI system with meta-learning and transfer capabilities"""
    
    def __init__(self):
        self.agents = {}
        self.environments = {}
        self.task_relationships = {}  # Track which tasks are similar
        self.global_experience = []   # Shared experience across tasks
        
    def add_task(self, task_name: str, environment, related_tasks: List[str] = None):
        """Add task with relationship information"""
        self.environments[task_name] = environment
        
        state_size = environment.get_state_size()
        action_size = environment.get_action_size()
        
        # Create enhanced agent
        self.agents[task_name] = MetaLearningAgent(state_size, action_size, learning_rate=0.0005)
        
        # Transfer learning from related tasks
        if related_tasks:
            self.task_relationships[task_name] = related_tasks
            self._transfer_knowledge(task_name, related_tasks)
        
        print(f"üéØ Added smart task '{task_name}'")
        print(f"   State size: {state_size}, Actions: {action_size}")
        if related_tasks:
            print(f"   Transferring from: {related_tasks}")
    
    def _transfer_knowledge(self, new_task: str, source_tasks: List[str]):
        """Transfer knowledge between related tasks"""
        new_agent = self.agents[new_task]
        
        # Simple transfer: average weights from source tasks
        source_agents = [self.agents[task] for task in source_tasks if task in self.agents]
        
        if source_agents and hasattr(source_agents[0], 'q_network'):
            # Transfer compatible layers
            for layer_idx in range(min(len(new_agent.q_network.layers), 
                                     len(source_agents[0].q_network.layers))):
                source_layer = source_agents[0].q_network.layers[layer_idx]
                target_layer = new_agent.q_network.layers[layer_idx]
                
                # Only transfer if dimensions match
                if (source_layer['weights'].shape == target_layer['weights'].shape):
                    # Initialize with transferred weights (scaled down)
                    target_layer['weights'] = source_layer['weights'] * 0.5
                    target_layer['biases'] = source_layer['biases'] * 0.5
                    print(f"   Transferred layer {layer_idx} weights")
    
    def learn_task(self, task_name: str, episodes: int = 1000, curriculum_learning: bool = True):
        """Enhanced learning with curriculum and meta-learning"""
        if task_name not in self.agents:
            print(f"‚ùå Task '{task_name}' not found!")
            return
        
        print(f"\nüß† Learning Task: {task_name}")
        print(f"Episodes: {episodes}, Curriculum: {curriculum_learning}")
        
        agent = self.agents[task_name]
        environment = self.environments[task_name]
        
        # Multi-phase learning
        if curriculum_learning and episodes > 200:
            # Phase 1: Exploration (first 30%)
            exploration_episodes = int(episodes * 0.3)
            agent.epsilon = 1.0
            agent.epsilon_decay = 0.999
            print(f"üîç Phase 1: Exploration ({exploration_episodes} episodes)")
            agent.train(environment, exploration_episodes, verbose=True)
            
            # Phase 2: Exploitation (remaining 70%)
            exploitation_episodes = episodes - exploration_episodes
            agent.epsilon = max(0.1, agent.epsilon)
            agent.epsilon_decay = 0.9995
            print(f"üéØ Phase 2: Exploitation ({exploitation_episodes} episodes)")
            agent.train(environment, exploitation_episodes, verbose=True)
        else:
            agent.train(environment, episodes, verbose=True)
        
        # Store experience for potential transfer
        self.global_experience.append({
            'task': task_name,
            'performance': np.mean(agent.episode_rewards[-50:]) if agent.episode_rewards else 0,
            'episodes': len(agent.episode_rewards)
        })
        
        print(f"‚úÖ Finished learning '{task_name}'!")
        self._analyze_learning(task_name)
    
    def _analyze_learning(self, task_name: str):
        """Analyze learning progress and provide insights"""
        agent = self.agents[task_name]
        
        if not agent.episode_rewards:
            return
        
        # Learning curve analysis
        early_performance = np.mean(agent.episode_rewards[:50]) if len(agent.episode_rewards) >= 50 else np.mean(agent.episode_rewards[:len(agent.episode_rewards)//2])
        late_performance = np.mean(agent.episode_rewards[-50:])
        improvement = late_performance - early_performance
        
        # Stability analysis
        recent_std = np.std(agent.episode_rewards[-50:]) if len(agent.episode_rewards) >= 50 else np.std(agent.episode_rewards)
        
        print(f"\nüìä Learning Analysis for {task_name}:")
        print(f"   Improvement: {improvement:.2f}")
        print(f"   Final Performance: {late_performance:.2f}")
        print(f"   Stability (std): {recent_std:.2f}")
        print(f"   States Explored: {len(agent.visit_counts)}")
        print(f"   Average TD Error: {np.mean(agent.td_errors[-100:]) if agent.td_errors else 'N/A'}")
        
        # Learning efficiency
        if len(agent.episode_rewards) > 100:
            learning_rate = (late_performance - early_performance) / len(agent.episode_rewards)
            print(f"   Learning Rate: {learning_rate:.4f} reward/episode")
    
    def demonstrate_task(self, task_name: str, episodes: int = 3, detailed: bool = True):
        """Enhanced demonstration with analysis"""
        if task_name not in self.agents:
            print(f"‚ùå Task '{task_name}' not found!")
            return
        
        print(f"\nüé≠ Demonstrating: {task_name}")
        agent = self.agents[task_name]
        environment = self.environments[task_name]
        
        performance = agent.test(environment, episodes)
        
        if detailed:
            # Show learned strategy
            print(f"\nüß© Learned Strategy Analysis:")
            print(f"   Exploration rate: {agent.epsilon:.4f}")
            print(f"   Memory size: {len(agent.memory)}")
            print(f"   Network depth: {len(agent.q_network.layers)}")
            
            # Sample some Q-values from different states
            if hasattr(environment, 'reset'):
                sample_states = []
                for _ in range(5):
                    state = environment.reset()
                    sample_states.append(state)
                
                print(f"   Sample Q-values:")
                for i, state in enumerate(sample_states):
                    q_vals = agent.q_network.forward(state, training=False)
                    best_action = np.argmax(q_vals)
                    print(f"     State {i}: Best action {best_action}, Q-max: {np.max(q_vals):.3f}")
        
        return performance
    
    def compare_tasks(self, task1: str, task2: str):
        """Compare learning performance between tasks"""
        if task1 not in self.agents or task2 not in self.agents:
            print("‚ùå One or both tasks not found!")
            return
        
        agent1 = self.agents[task1]
        agent2 = self.agents[task2]
        
        print(f"\n‚öñÔ∏è  Comparing {task1} vs {task2}:")
        
        # Performance comparison
        perf1 = np.mean(agent1.episode_rewards[-50:]) if agent1.episode_rewards else 0
        perf2 = np.mean(agent2.episode_rewards[-50:]) if agent2.episode_rewards else 0
        
        print(f"   Performance: {task1}: {perf1:.2f}, {task2}: {perf2:.2f}")
        
        # Learning speed comparison
        episodes1 = len(agent1.episode_rewards)
        episodes2 = len(agent2.episode_rewards)
        
        if episodes1 > 0 and episodes2 > 0:
            speed1 = perf1 / episodes1 if episodes1 > 0 else 0
            speed2 = perf2 / episodes2 if episodes2 > 0 else 0
            print(f"   Learning Speed: {task1}: {speed1:.4f}, {task2}: {speed2:.4f}")
        
        # Exploration comparison
        states1 = len(agent1.visit_counts)
        states2 = len(agent2.visit_counts)
        print(f"   States Explored: {task1}: {states1}, {task2}: {states2}")
    
    def get_smart_insights(self):
        """Provide intelligent insights about learning progress"""
        print(f"\nüî¨ Smart AI Insights:")
        print(f"   Total tasks learned: {len(self.agents)}")
        
        # Overall performance ranking
        task_performances = []
        for task_name, agent in self.agents.items():
            if agent.episode_rewards:
                avg_performance = np.mean(agent.episode_rewards[-50:])
                task_performances.append((task_name, avg_performance))
        
        task_performances.sort(key=lambda x: x[1], reverse=True)
        
        if task_performances:
            print(f"   Best performing task: {task_performances[0][0]} ({task_performances[0][1]:.2f})")
            print(f"   Most challenging task: {task_performances[-1][0]} ({task_performances[-1][1]:.2f})")
        
        # Learning efficiency insights
        total_episodes = sum(len(agent.episode_rewards) for agent in self.agents.values())
        total_exploration = sum(len(agent.visit_counts) for agent in self.agents.values())
        
        print(f"   Total learning episodes: {total_episodes}")
        print(f"   Total states explored: {total_exploration}")
        print(f"   Average exploration per episode: {total_exploration/total_episodes:.2f}")
        
        # Transfer learning opportunities
        similar_tasks = []
        for task, related in self.task_relationships.items():
            if related:
                similar_tasks.append(f"{task} ‚Üê {', '.join(related)}")
        
        if similar_tasks:
            print(f"   Transfer learning used: {len(similar_tasks)} relationships")
            for relationship in similar_tasks:
                print(f"     {relationship}")

# Example usage with enhanced capabilities
if __name__ == "__main__":
    print("üöÄ Enhanced Smart AI Learning System")
    print("=" * 50)
    
    # Create the enhanced AI
    smart_ai = SmartAI()
    
    print("\nüèóÔ∏è  Setting Up Advanced Learning Environment...")
    
    # Add enhanced tasks
    advanced_grid = AdvancedGridWorld(width=8, height=8)
    smart_ai.add_task("advanced_navigation", advanced_grid)
    
    # Add a simpler navigation task for transfer learning
    simple_grid = AdvancedGridWorld(width=5, height=5)
    smart_ai.add_task("simple_navigation", simple_grid)
    
    # Create navigation task with transfer learning
    complex_grid = AdvancedGridWorld(width=10, height=10)
    smart_ai.add_task("complex_navigation", complex_grid, 
                     related_tasks=["simple_navigation", "advanced_navigation"])
    
    print("\nüéì Starting Multi-Phase Learning...")
    
    # Train with curriculum learning
    smart_ai.learn_task("simple_navigation", episodes=300)
    smart_ai.learn_task("advanced_navigation", episodes=500)
    smart_ai.learn_task("complex_navigation", episodes=400)  # Should learn faster due to transfer
    
    print("\nüìà Performance Analysis...")
    smart_ai.compare_tasks("simple_navigation", "advanced_navigation")
    smart_ai.compare_tasks("advanced_navigation", "complex_navigation")
    
    print("\nüéØ Demonstrating Enhanced Capabilities...")
    smart_ai.demonstrate_task("complex_navigation", episodes=2, detailed=True)
    
    # Get overall insights
    smart_ai.get_smart_insights()
    
    print("\nüéâ Enhanced AI Features Demonstrated:")
    print("‚úÖ Prioritized Experience Replay")
    print("‚úÖ Double DQN with Target Networks") 
    print("‚úÖ Curiosity-Driven Learning")
    print("‚úÖ Multi-Step Learning")
    print("‚úÖ Adaptive Exploration")
    print("‚úÖ Transfer Learning")
    print("‚úÖ Curriculum Learning")
    print("‚úÖ Meta-Learning Capabilities")
    print("‚úÖ Advanced Neural Networks (Adam optimizer)")
    print("‚úÖ Intrinsic Motivation")
    print("‚úÖ Learning Analytics")
    
    print(f"\nüß† The AI is now significantly smarter with:")
    print(f"   - Better exploration strategies")
    print(f"   - Faster learning through experience prioritization")  
    print(f"   - Knowledge transfer between tasks")
    print(f"   - Curiosity-driven discovery")
    print(f"   - Adaptive learning rates")
    print(f"   - Meta-cognitive analysis")
