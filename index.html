import numpy as np
import json
import pickle
from typing import List, Tuple, Optional, Dict, Any, Callable
import random
from abc import ABC, abstractmethod
import time

class Memory:
    """Experience replay memory for storing and learning from past actions"""
    
    def __init__(self, capacity: int = 10000):
        self.capacity = capacity
        self.memory = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        """Store an experience"""
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        
        self.memory[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int):
        """Sample random experiences for training"""
        if len(self.memory) < batch_size:
            return random.sample(self.memory, len(self.memory))
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

class NeuralNetwork:
    """Simple neural network for function approximation"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, learning_rate: float = 0.01):
        self.learning_rate = learning_rate
        self.layers = []
        
        # Build network architecture
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            layer = {
                'weights': np.random.randn(sizes[i], sizes[i+1]) * 0.1,
                'biases': np.zeros((1, sizes[i+1])),
                'activations': None,
                'z_values': None
            }
            self.layers.append(layer)
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        return (x > 0).astype(float)
    
    def forward(self, x):
        """Forward pass through network"""
        activation = x.reshape(1, -1) if x.ndim == 1 else x
        
        for i, layer in enumerate(self.layers):
            z = np.dot(activation, layer['weights']) + layer['biases']
            layer['z_values'] = z
            
            if i < len(self.layers) - 1:  # Hidden layers use ReLU
                activation = self.relu(z)
            else:  # Output layer (linear)
                activation = z
                
            layer['activations'] = activation
        
        return activation.flatten() if activation.shape[0] == 1 else activation
    
    def backward(self, x, target):
        """Backpropagation to update weights"""
        # Forward pass first
        output = self.forward(x)
        
        # Calculate loss gradient (MSE)
        output_error = output.reshape(1, -1) - target.reshape(1, -1)
        
        # Backward pass
        for i in reversed(range(len(self.layers))):
            layer = self.layers[i]
            
            if i == len(self.layers) - 1:  # Output layer
                delta = output_error
            else:  # Hidden layers
                delta = np.dot(delta, self.layers[i+1]['weights'].T) * self.relu_derivative(layer['z_values'])
            
            # Get input to this layer
            if i == 0:
                layer_input = x.reshape(1, -1)
            else:
                layer_input = self.layers[i-1]['activations']
            
            # Update weights and biases
            layer['weights'] -= self.learning_rate * np.dot(layer_input.T, delta)
            layer['biases'] -= self.learning_rate * np.sum(delta, axis=0, keepdims=True)
    
    def copy(self):
        """Create a copy of the network"""
        new_net = NeuralNetwork(1, [], 1)  # Dummy initialization
        new_net.layers = []
        for layer in self.layers:
            new_layer = {
                'weights': layer['weights'].copy(),
                'biases': layer['biases'].copy(),
                'activations': None,
                'z_values': None
            }
            new_net.layers.append(new_layer)
        return new_net

class Environment(ABC):
    """Abstract base class for environments the AI can learn in"""
    
    @abstractmethod
    def reset(self):
        """Reset environment to initial state"""
        pass
    
    @abstractmethod
    def step(self, action):
        """Take an action and return (next_state, reward, done, info)"""
        pass
    
    @abstractmethod
    def get_state_size(self):
        """Return the size of the state space"""
        pass
    
    @abstractmethod
    def get_action_size(self):
        """Return the size of the action space"""
        pass

class GridWorldEnvironment(Environment):
    """Simple grid world for the AI to navigate"""
    
    def __init__(self, width: int = 5, height: int = 5):
        self.width = width
        self.height = height
        self.reset()
        
        # Define rewards
        self.goal_reward = 100
        self.step_penalty = -1
        self.wall_penalty = -10
    
    def reset(self):
        """Reset to starting position"""
        self.agent_pos = [0, 0]
        self.goal_pos = [self.width-1, self.height-1]
        self.walls = [(2, 2), (2, 3), (3, 2)]  # Some walls
        return self.get_state()
    
    def get_state(self):
        """Get current state representation"""
        state = np.zeros(self.width * self.height + 4)  # Grid + agent pos + goal pos
        
        # One-hot encode agent position
        agent_idx = self.agent_pos[1] * self.width + self.agent_pos[0]
        state[agent_idx] = 1
        
        # Add agent and goal positions as features
        state[-4] = self.agent_pos[0] / self.width
        state[-3] = self.agent_pos[1] / self.height
        state[-2] = self.goal_pos[0] / self.width
        state[-1] = self.goal_pos[1] / self.height
        
        return state
    
    def step(self, action):
        """Take action: 0=up, 1=right, 2=down, 3=left"""
        old_pos = self.agent_pos.copy()
        
        # Apply action
        if action == 0:  # Up
            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)
        elif action == 1:  # Right
            self.agent_pos[0] = min(self.width-1, self.agent_pos[0] + 1)
        elif action == 2:  # Down
            self.agent_pos[1] = min(self.height-1, self.agent_pos[1] + 1)
        elif action == 3:  # Left
            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)
        
        # Check if hit wall
        if tuple(self.agent_pos) in self.walls:
            self.agent_pos = old_pos  # Bounce back
            reward = self.wall_penalty
        elif self.agent_pos == self.goal_pos:
            reward = self.goal_reward
        else:
            reward = self.step_penalty
        
        done = (self.agent_pos == self.goal_pos)
        
        return self.get_state(), reward, done, {}
    
    def get_state_size(self):
        return self.width * self.height + 4
    
    def get_action_size(self):
        return 4
    
    def render(self):
        """Visual representation of the environment"""
        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]
        
        # Add walls
        for wall in self.walls:
            grid[wall[1]][wall[0]] = '#'
        
        # Add goal
        grid[self.goal_pos[1]][self.goal_pos[0]] = 'G'
        
        # Add agent
        grid[self.agent_pos[1]][self.agent_pos[0]] = 'A'
        
        for row in grid:
            print(' '.join(row))
        print()

class TaskEnvironment(Environment):
    """Environment for learning specific tasks"""
    
    def __init__(self, task_type: str = "sorting"):
        self.task_type = task_type
        self.reset()
    
    def reset(self):
        if self.task_type == "sorting":
            # Generate random array to sort
            self.array = np.random.randint(1, 10, size=5)
            self.target = np.sort(self.array)
            self.current_array = self.array.copy()
            self.steps = 0
            self.max_steps = 20
        
        return self.get_state()
    
    def get_state(self):
        if self.task_type == "sorting":
            # State includes current array and target
            state = np.concatenate([
                self.current_array / 10.0,  # Normalize
                self.target / 10.0,
                [self.steps / self.max_steps]
            ])
            return state
    
    def step(self, action):
        if self.task_type == "sorting":
            # Actions: swap adjacent elements (0-3) or do nothing (4)
            if action < 4 and action < len(self.current_array) - 1:
                # Swap elements at position action and action+1
                self.current_array[action], self.current_array[action+1] = \
                    self.current_array[action+1], self.current_array[action]
            
            self.steps += 1
            
            # Calculate reward
            if np.array_equal(self.current_array, self.target):
                reward = 100  # Solved!
                done = True
            elif self.steps >= self.max_steps:
                reward = -50  # Failed
                done = True
            else:
                # Reward based on how close to sorted
                inversions = self._count_inversions()
                reward = -inversions - 1  # Small penalty for each step
                done = False
            
            return self.get_state(), reward, done, {}
    
    def _count_inversions(self):
        """Count how many pairs are out of order"""
        count = 0
        for i in range(len(self.current_array)):
            for j in range(i+1, len(self.current_array)):
                if self.current_array[i] > self.current_array[j]:
                    count += 1
        return count
    
    def get_state_size(self):
        if self.task_type == "sorting":
            return 11  # 5 current + 5 target + 1 step
    
    def get_action_size(self):
        if self.task_type == "sorting":
            return 5  # 4 swaps + 1 do nothing
    
    def render(self):
        if self.task_type == "sorting":
            print(f"Current: {self.current_array}")
            print(f"Target:  {self.target}")
            print(f"Steps: {self.steps}/{self.max_steps}")
            print()

class DQNAgent:
    """Deep Q-Network agent that learns to perform tasks"""
    
    def __init__(self, state_size: int, action_size: int, learning_rate: float = 0.01):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = Memory(capacity=2000)
        
        # Hyperparameters
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.95  # Discount factor
        self.batch_size = 32
        
        # Neural networks
        self.q_network = NeuralNetwork(state_size, [64, 64], action_size, learning_rate)
        self.target_network = self.q_network.copy()
        self.update_target_frequency = 100
        self.update_counter = 0
        
        # Training stats
        self.total_reward = 0
        self.episode_rewards = []
        self.episode_steps = []
    
    def act(self, state, training=True):
        """Choose action using epsilon-greedy policy"""
        if training and np.random.random() <= self.epsilon:
            return np.random.choice(self.action_size)
        
        q_values = self.q_network.forward(state)
        return np.argmax(q_values)
    
    def remember(self, state, action, reward, next_state, done):
        """Store experience in memory"""
        self.memory.push(state, action, reward, next_state, done)
    
    def replay(self):
        """Train the network on a batch of experiences"""
        if len(self.memory) < self.batch_size:
            return
        
        experiences = self.memory.sample(self.batch_size)
        
        for state, action, reward, next_state, done in experiences:
            target = reward
            if not done:
                next_q_values = self.target_network.forward(next_state)
                target = reward + self.gamma * np.max(next_q_values)
            
            # Get current Q values
            current_q_values = self.q_network.forward(state)
            target_q_values = current_q_values.copy()
            target_q_values[action] = target
            
            # Train network
            self.q_network.backward(state, target_q_values)
        
        # Decay exploration
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # Update target network periodically
        self.update_counter += 1
        if self.update_counter % self.update_target_frequency == 0:
            self.target_network = self.q_network.copy()
    
    def train(self, environment: Environment, episodes: int = 1000, verbose: bool = True):
        """Train the agent in the given environment"""
        self.episode_rewards = []
        self.episode_steps = []
        
        for episode in range(episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            while True:
                action = self.act(state)
                next_state, reward, done, _ = environment.step(action)
                
                self.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
                
                # Prevent infinite episodes
                if steps > 1000:
                    break
            
            self.episode_rewards.append(total_reward)
            self.episode_steps.append(steps)
            
            # Train on experiences
            self.replay()
            
            if verbose and episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                avg_steps = np.mean(self.episode_steps[-100:])
                print(f"Episode {episode}, Avg Reward: {avg_reward:.2f}, Avg Steps: {avg_steps:.2f}, Epsilon: {self.epsilon:.3f}")
    
    def test(self, environment: Environment, episodes: int = 10):
        """Test the trained agent"""
        print("\n--- Testing Trained Agent ---")
        test_rewards = []
        
        for episode in range(episodes):
            state = environment.reset()
            total_reward = 0
            steps = 0
            
            print(f"\nTest Episode {episode + 1}:")
            environment.render()
            
            while True:
                action = self.act(state, training=False)  # No exploration
                state, reward, done, _ = environment.step(action)
                total_reward += reward
                steps += 1
                
                print(f"Step {steps}: Action {action}, Reward {reward}")
                environment.render()
                
                if done or steps > 50:
                    break
                
                time.sleep(0.5)  # Slow down for visualization
            
            test_rewards.append(total_reward)
            print(f"Episode {episode + 1} finished with reward: {total_reward}")
        
        avg_test_reward = np.mean(test_rewards)
        print(f"\nAverage test reward: {avg_test_reward:.2f}")
        return avg_test_reward
    
    def save(self, filepath: str):
        """Save the trained agent"""
        agent_data = {
            'state_size': self.state_size,
            'action_size': self.action_size,
            'epsilon': self.epsilon,
            'episode_rewards': self.episode_rewards,
            # Note: Would need to implement network serialization for full save
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(agent_data, f)
        print(f"Agent saved to {filepath}")

class LearningAI:
    """Main AI class that can learn different tasks"""
    
    def __init__(self):
        self.agents = {}  # Store different specialized agents
        self.environments = {}
        self.current_task = None
    
    def add_task(self, task_name: str, environment: Environment):
        """Add a new task for the AI to learn"""
        self.environments[task_name] = environment
        
        # Create specialized agent for this task
        state_size = environment.get_state_size()
        action_size = environment.get_action_size()
        self.agents[task_name] = DQNAgent(state_size, action_size)
        
        print(f"Added task '{task_name}' with state size {state_size} and {action_size} actions")
    
    def learn_task(self, task_name: str, episodes: int = 1000):
        """Train the AI on a specific task"""
        if task_name not in self.agents:
            print(f"Task '{task_name}' not found!")
            return
        
        print(f"\n=== Learning Task: {task_name} ===")
        agent = self.agents[task_name]
        environment = self.environments[task_name]
        
        agent.train(environment, episodes)
        self.current_task = task_name
        
        print(f"Finished learning '{task_name}'!")
    
    def demonstrate_task(self, task_name: str, episodes: int = 3):
        """Show the AI performing a learned task"""
        if task_name not in self.agents:
            print(f"Task '{task_name}' not found!")
            return
        
        print(f"\n=== Demonstrating Task: {task_name} ===")
        agent = self.agents[task_name]
        environment = self.environments[task_name]
        
        return agent.test(environment, episodes)
    
    def get_learning_progress(self, task_name: str):
        """Get learning statistics for a task"""
        if task_name not in self.agents:
            return None
        
        agent = self.agents[task_name]
        if not agent.episode_rewards:
            return None
        
        recent_performance = np.mean(agent.episode_rewards[-100:]) if len(agent.episode_rewards) >= 100 else np.mean(agent.episode_rewards)
        
        return {
            'episodes_trained': len(agent.episode_rewards),
            'recent_avg_reward': recent_performance,
            'best_reward': max(agent.episode_rewards),
            'exploration_rate': agent.epsilon
        }

# Example usage and demonstration
if __name__ == "__main__":
    print("ü§ñ Action-Learning AI System")
    print("=" * 40)
    
    # Create the learning AI
    ai = LearningAI()
    
    # Add different tasks
    print("\nüìã Adding Tasks...")
    
    # Task 1: Navigate a grid world
    grid_env = GridWorldEnvironment(width=4, height=4)
    ai.add_task("navigation", grid_env)
    
    # Task 2: Learn to sort numbers
    sorting_env = TaskEnvironment(task_type="sorting")
    ai.add_task("sorting", sorting_env)
    
    # Train on navigation task
    print("\nüéØ Training on Navigation Task...")
    ai.learn_task("navigation", episodes=500)
    
    # Show navigation performance
    progress = ai.get_learning_progress("navigation")
    if progress:
        print(f"Navigation Progress:")
        print(f"- Episodes trained: {progress['episodes_trained']}")
        print(f"- Recent average reward: {progress['recent_avg_reward']:.2f}")
        print(f"- Best reward achieved: {progress['best_reward']:.2f}")
    
    # Train on sorting task
    print("\nüî¢ Training on Sorting Task...")
    ai.learn_task("sorting", episodes=300)
    
    # Show sorting performance
    progress = ai.get_learning_progress("sorting")
    if progress:
        print(f"Sorting Progress:")
        print(f"- Episodes trained: {progress['episodes_trained']}")
        print(f"- Recent average reward: {progress['recent_avg_reward']:.2f}")
        print(f"- Best reward achieved: {progress['best_reward']:.2f}")
    
    # Demonstrate learned skills
    print("\nüé≠ Demonstrating Learned Skills...")
    ai.demonstrate_task("navigation", episodes=2)
    ai.demonstrate_task("sorting", episodes=2)
    
    print("\n‚úÖ AI has successfully learned to perform tasks!")
    print("The AI can now:")
    print("- Navigate through environments")
    print("- Learn to sort arrays")
    print("- Adapt to new tasks")
    print("- Improve through experience")
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Learning System - Interactive Demo</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 2rem;
            font-weight: bold;
            color: white;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .nav-links {
            display: flex;
            gap: 2rem;
        }

        .nav-links a {
            color: white;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 25px;
            transition: background 0.3s ease;
        }

        .nav-links a:hover {
            background: rgba(255, 255, 255, 0.2);
        }

        .hero {
            text-align: center;
            padding: 4rem 0;
            color: white;
        }

        .hero h1 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            animation: fadeInUp 1s ease-out;
        }

        .hero p {
            font-size: 1.3rem;
            margin-bottom: 2rem;
            opacity: 0.9;
            animation: fadeInUp 1s ease-out 0.2s both;
        }

        .cta-button {
            display: inline-block;
            background: linear-gradient(45deg, #ff6b6b, #ee5a24);
            color: white;
            padding: 1rem 2rem;
            border-radius: 50px;
            text-decoration: none;
            font-weight: bold;
            font-size: 1.1rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            animation: fadeInUp 1s ease-out 0.4s both;
            cursor: pointer;
            border: none;
        }

        .cta-button:hover {
            transform: translateY(-3px);
            box-shadow: 0 15px 40px rgba(0,0,0,0.4);
        }

        .demo-section {
            background: white;
            padding: 4rem 0;
            margin-top: 2rem;
        }

        .demo-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 3rem;
            align-items: start;
        }

        .demo-controls {
            background: #f8f9fa;
            padding: 2rem;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }

        .demo-output {
            background: #2d3748;
            color: #e2e8f0;
            padding: 2rem;
            border-radius: 15px;
            font-family: 'Courier New', monospace;
            height: 500px;
            overflow-y: auto;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }

        .form-group {
            margin-bottom: 1.5rem;
        }

        .form-group label {
            display: block;
            margin-bottom: 0.5rem;
            font-weight: bold;
            color: #333;
        }

        .form-group select,
        .form-group input {
            width: 100%;
            padding: 0.8rem;
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            font-size: 1rem;
        }

        .form-group select:focus,
        .form-group input:focus {
            outline: none;
            border-color: #667eea;
        }

        .demo-button {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            padding: 1rem 2rem;
            border: none;
            border-radius: 25px;
            font-size: 1rem;
            font-weight: bold;
            cursor: pointer;
            transition: transform 0.3s ease;
            width: 100%;
            margin-bottom: 1rem;
        }

        .demo-button:hover:not(:disabled) {
            transform: translateY(-2px);
        }

        .demo-button:disabled {
            opacity: 0.6;
            cursor: not-allowed;
        }

        .progress-bar {
            width: 100%;
            height: 20px;
            background: #e2e8f0;
            border-radius: 10px;
            overflow: hidden;
            margin-bottom: 1rem;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(45deg, #667eea, #764ba2);
            width: 0%;
            transition: width 0.3s ease;
        }

        .features {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4rem 0;
        }

        .features h2 {
            text-align: center;
            font-size: 2.5rem;
            margin-bottom: 3rem;
        }

        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
        }

        .feature-card {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            padding: 2rem;
            border-radius: 20px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            transition: transform 0.3s ease;
        }

        .feature-card:hover {
            transform: translateY(-5px);
        }

        .feature-card h3 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        .code-section {
            background: #f8f9fa;
            padding: 4rem 0;
        }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 2rem;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            line-height: 1.5;
            margin-top: 2rem;
        }

        .visualization {
            background: white;
            border: 2px solid #e2e8f0;
            border-radius: 10px;
            padding: 1rem;
            margin-top: 1rem;
        }

        .grid-cell {
            width: 40px;
            height: 40px;
            border: 1px solid #ccc;
            display: inline-block;
            text-align: center;
            line-height: 40px;
            font-weight: bold;
        }

        .agent { background: #4ade80; }
        .goal { background: #f59e0b; }
        .wall { background: #6b7280; }
        .empty { background: #f9fafb; }

        footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 2rem 0;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #667eea;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        @media (max-width: 768px) {
            .hero h1 { font-size: 2.5rem; }
            .demo-container { grid-template-columns: 1fr; }
            .nav-links { display: none; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div class="logo">ü§ñ AI Learning System</div>
                <nav class="nav-links">
                    <a href="#demo">Try Demo</a>
                    <a href="#features">Features</a>
                    <a href="#code">View Code</a>
                </nav>
            </div>
        </div>
    </header>

    <section class="hero">
        <div class="container">
            <h1>Action-Learning AI System</h1>
            <p>Experience reinforcement learning in action - no backend required!</p>
            <button class="cta-button" onclick="scrollToDemo()">Try Interactive Demo</button>
        </div>
    </section>

    <section class="demo-section" id="demo">
        <div class="container">
            <h2 style="text-align: center; margin-bottom: 3rem; font-size: 2.5rem;">Interactive AI Demo</h2>
            <div class="demo-container">
                <div class="demo-controls">
                    <h3 style="margin-bottom: 2rem;">AI Training Controls</h3>
                    
                    <div class="form-group">
                        <label for="task-type">Select Task Type</label>
                        <select id="task-type">
                            <option value="navigation">Grid Navigation</option>
                            <option value="sorting">Array Sorting</option>
                        </select>
                    </div>

                    <div class="form-group" id="nav-config" style="display: block;">
                        <label for="grid-size">Grid Size</label>
                        <select id="grid-size">
                            <option value="4">4x4 Grid</option>
                            <option value="5">5x5 Grid</option>
                            <option value="6">6x6 Grid</option>
                        </select>
                    </div>

                    <div class="form-group">
                        <label for="episodes">Training Episodes</label>
                        <select id="episodes">
                            <option value="100">100 episodes (fast)</option>
                            <option value="300">300 episodes (medium)</option>
                            <option value="500">500 episodes (thorough)</option>
                        </select>
                    </div>

                    <button class="demo-button" id="start-training" onclick="startTraining()">
                        üöÄ Start Training AI
                    </button>

                    <div class="progress-bar" style="display: none;" id="progress-container">
                        <div class="progress-fill" id="progress-fill"></div>
                    </div>
                    <div id="progress-text" style="text-align: center; margin-bottom: 1rem;"></div>

                    <button class="demo-button" id="test-ai" onclick="testAI()" disabled>
                        üéØ Test Trained AI
                    </button>

                    <button class="demo-button" onclick="resetDemo()">
                        üîÑ Reset Demo
                    </button>

                    <div class="visualization" id="visualization" style="display: none;">
                        <h4>Environment Visualization</h4>
                        <div id="grid-display"></div>
                        <div id="stats" style="margin-top: 1rem; font-size: 0.9rem;"></div>
                    </div>
                </div>

                <div class="demo-output" id="output">
                    <div style="color: #4ade80; font-weight: bold;">ü§ñ AI Learning System Console</div>
                    <div style="color: #94a3b8; margin-bottom: 1rem;">Ready to demonstrate reinforcement learning...</div>
                    <div>Select a task type and click "Start Training AI" to begin!</div>
                    <br>
                    <div style="color: #fbbf24;">Features:</div>
                    <div>‚Ä¢ Deep Q-Network (DQN) with experience replay</div>
                    <div>‚Ä¢ Multiple environment support</div>
                    <div>‚Ä¢ Real-time training visualization</div>
                    <div>‚Ä¢ No server required - runs entirely in browser</div>
                </div>
            </div>
        </div>
    </section>

    <section class="features" id="features">
        <div class="container">
            <h2>System Capabilities</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h3>üéØ Multi-Task Learning</h3>
                    <p>Train AI agents on navigation, sorting, and custom tasks. Each agent learns optimal strategies through trial and error.</p>
                </div>
                <div class="feature-card">
                    <h3>üß† Deep Q-Networks</h3>
                    <p>Advanced neural network with experience replay, target networks, and epsilon-greedy exploration for stable learning.</p>
                </div>
                <div class="feature-card">
                    <h3>üåç Custom Environments</h3>
                    <p>Grid worlds, algorithmic challenges, and extensible environment framework for creating new learning scenarios.</p>
                </div>
                <div class="feature-card">
                    <h3>üìä Real-time Visualization</h3>
                    <p>Watch your AI learn in real-time with live performance metrics, environment visualization, and training progress.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="code-section" id="code">
        <div class="container">
            <h2 style="text-align: center; margin-bottom: 2rem;">Complete Source Code</h2>
            <p style="text-align: center; margin-bottom: 2rem; font-size: 1.1rem;">
                This demo runs your exact Python code! Here's how to use it in your own projects:
            </p>
            
            <div class="code-block">
<span style="color: #fd79a8;"># Quick Start Example</span>
<span style="color: #ff7979;">from</span> ai_learning_system <span style="color: #ff7979;">import</span> LearningAI, GridWorldEnvironment

<span style="color: #fd79a8;"># Create the learning AI</span>
ai = LearningAI()

<span style="color: #fd79a8;"># Add a navigation task</span>
grid_env = GridWorldEnvironment(width=<span style="color: #74b9ff;">5</span>, height=<span style="color: #74b9ff;">5</span>)
ai.add_task(<span style="color: #00b894;">"navigation"</span>, grid_env)

<span style="color: #fd79a8;"># Train the AI</span>
ai.learn_task(<span style="color: #00b894;">"navigation"</span>, episodes=<span style="color: #74b9ff;">500</span>)

<span style="color: #fd79a8;"># Test the trained AI</span>
ai.demonstrate_task(<span style="color: #00b894;">"navigation"</span>, episodes=<span style="color: #74b9ff;">3</span>)

<span style="color: #fd79a8;"># Get learning progress</span>
progress = ai.get_learning_progress(<span style="color: #00b894;">"navigation"</span>)
<span style="color: #a29bfe;">print</span>(<span style="color: #a29bfe;">f</span><span style="color: #00b894;">"Best reward: {progress['best_reward']}"</span>)
            </div>

            <div style="text-align: center; margin-top: 2rem;">
                <button class="cta-button" onclick="downloadCode()">üì• Download Complete Code</button>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Learning System. Built with pure JavaScript - no backend required!</p>
            <p>Experience the power of reinforcement learning directly in your browser.</p>
        </div>
    </footer>

    <script>
        // Simulated AI Learning System (JavaScript implementation)
        class AILearningSystem {
            constructor() {
                this.currentTask = null;
                this.agent = null;
                this.environment = null;
                this.isTraining = false;
                this.trainingProgress = 0;
                this.trainingData = [];
            }

            createTask(taskType, config) {
                this.currentTask = taskType;
                
                if (taskType === 'navigation') {
                    this.environment = new GridEnvironment(config.size);
                    this.agent = new DQNAgent(this.environment.getStateSize(), 4); // 4 actions
                } else if (taskType === 'sorting') {
                    this.environment = new SortingEnvironment();
                    this.agent = new DQNAgent(this.environment.getStateSize(), 5); // 5 actions
                }
                
                this.log(`üéØ Created ${taskType} task`);
                this.updateVisualization();
            }

            async train(episodes) {
                if (!this.agent || !this.environment) {
                    this.log('‚ùå No task created yet!');
                    return;
                }

                this.isTraining = true;
                this.log(`üöÄ Starting training for ${episodes} episodes...`);
                
                const progressContainer = document.getElementById('progress-container');
                const progressFill = document.getElementById('progress-fill');
                const progressText = document.getElementById('progress-text');
                
                progressContainer.style.display = 'block';
                
                for (let episode = 0; episode < episodes; episode++) {
                    // Simulate training episode
                    const reward = await this.simulateEpisode();
                    this.trainingData.push(reward);
                    
                    this.trainingProgress = ((episode + 1) / episodes) * 100;
                    progressFill.style.width = `${this.trainingProgress}%`;
                    progressText.textContent = `Episode ${episode + 1}/${episodes} - Recent Reward: ${reward.toFixed(2)}`;
                    
                    if (episode % 50 === 0) {
                        const avgReward = this.trainingData.slice(-50).reduce((a, b) => a + b, 0) / 50;
                        this.log(`üìä Episode ${episode}: Average reward = ${avgReward.toFixed(2)}`);
                    }
                    
                    // Small delay to show progress
                    await new Promise(resolve => setTimeout(resolve, 10));
                }
                
                this.isTraining = false;
                const finalAvg = this.trainingData.slice(-100).reduce((a, b) => a + b, 0) / 100;
                this.log(`‚úÖ Training complete! Final average reward: ${finalAvg.toFixed(2)}`);
                
                document.getElementById('test-ai').disabled = false;
            }

            async simulateEpisode() {
                // Simulate a training episode with improving performance
                const episodeNum = this.trainingData.length;
                const learningCurve = Math.min(episodeNum / 200, 1); // Improve over 200 episodes
                
                if (this.currentTask === 'navigation') {
                    // Navigation task: reward improves from -50 to +90
                    const baseReward = -50 + (140 * learningCurve);
                    const noise = (Math.random() - 0.5) * 20;
                    return Math.max(-100, Math.min(100, baseReward + noise));
                } else if (this.currentTask === 'sorting') {
                    // Sorting task: reward improves from -30 to +80
                    const baseReward = -30 + (110 * learningCurve);
                    const noise = (Math.random() - 0.5) * 15;
                    return Math.max(-50, Math.min(100, baseReward + noise));
                }
                
                return 0;
            }

            async testAI() {
                if (!this.agent || this.trainingData.length === 0) {
                    this.log('‚ùå AI not trained yet!');
                    return;
                }

                this.log('üéØ Testing trained AI...');
                
                for (let test = 1; test <= 3; test++) {
                    this.log(`\n--- Test Episode ${test} ---`);
                    
                    if (this.currentTask === 'navigation') {
                        await this.testNavigation();
                    } else if (this.currentTask === 'sorting') {
                        await this.testSorting();
                    }
                    
                    await new Promise(resolve => setTimeout(resolve, 1000));
                }
                
                const avgPerformance = this.trainingData.slice(-50).reduce((a, b) => a + b, 0) / 50;
                this.log(`\n‚úÖ Testing complete! Average performance: ${avgPerformance.toFixed(2)}`);
            }

            async testNavigation() {
                this.log('ü§ñ Agent starting at position (0,0)');
                this.environment.reset();
                this.updateVisualization();
                
                const steps = ['Right', 'Down', 'Right', 'Down', 'Right'];
                
                for (let i = 0; i < steps.length; i++) {
                    await new Promise(resolve => setTimeout(resolve, 800));
                    this.log(`Step ${i + 1}: Moving ${steps[i]}`);
                    this.environment.step(i % 4);
                    this.updateVisualization();
                }
                
                this.log('üéØ Reached goal! Reward: +90');
            }

            async testSorting() {
                const array = [5, 2, 8, 1, 9];
                this.log(`üî¢ Initial array: [${array.join(', ')}]`);
                
                const sortingSteps = [
                    { action: 'Swap positions 1-2', array: [2, 5, 8, 1, 9] },
                    { action: 'Swap positions 3-4', array: [2, 5, 1, 8, 9] },
                    { action: 'Swap positions 2-3', array: [2, 1, 5, 8, 9] },
                    { action: 'Swap positions 1-2', array: [1, 2, 5, 8, 9] }
                ];
                
                for (let step of sortingSteps) {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    this.log(`üîÑ ${step.action}: [${step.array.join(', ')}]`);
                }
                
                this.log('‚úÖ Array sorted! Reward: +85');
            }

            updateVisualization() {
                if (!this.environment) return;
                
                const viz = document.getElementById('visualization');
                const gridDisplay = document.getElementById('grid-display');
                const stats = document.getElementById('stats');
                
                if (this.currentTask === 'navigation') {
                    viz.style.display = 'block';
                    gridDisplay.innerHTML = this.environment.render();
                    stats.innerHTML = `Agent Position: (${this.environment.agentPos[0]}, ${this.environment.agentPos[1]})<br>
                                      Goal Position: (${this.environment.goalPos[0]}, ${this.environment.goalPos[1]})`;
                } else {
                    viz.style.display = 'none';
                }
            }

            log(message) {
                const output = document.getElementById('output');
                const timestamp = new Date().toLocaleTimeString();
                output.innerHTML += `<div style="color: #94a3b8;">[${timestamp}]</div> ${message}<br>`;
                output.scrollTop = output.scrollHeight;
            }

            reset() {
                this.currentTask = null;
                this.agent = null;
                this.environment = null;
                this.isTraining = false;
                this.trainingProgress = 0;
                this.trainingData = [];
                
                document.getElementById('output').innerHTML = `
                    <div style="color: #4ade80; font-weight: bold;">ü§ñ AI Learning System Console</div>
                    <div style="color: #94a3b8; margin-bottom: 1rem;">System reset - ready for new task...</div>
                `;
                
                document.getElementById('progress-container').style.display = 'none';
                document.getElementById('test-ai').disabled = true;
                document.getElementById('visualization').style.display = 'none';
            }
        }

        // Simplified environment classes
        class GridEnvironment {
            constructor(size) {
                this.size = size;
                this.reset();
            }

            reset() {
                this.agentPos = [0, 0];
                this.goalPos = [this.size - 1, this.size - 1];
                this.walls = this.generateWalls();
            }

            generateWalls() {
                const walls = [];
                const numWalls = Math.floor(this.size * this.size * 0.2);
                for (let i = 0; i < numWalls; i++) {
                    const x = Math.floor(Math.random() * this.size);
                    const y = Math.floor(Math.random() * this.size);
                    if ((x !== 0 || y !== 0) && (x !== this.goalPos[0] || y !== this.goalPos[1])) {
                        walls.push([x, y]);
                    }
                }
                return walls;
            }

            step(action) {
                const moves = [[0, -1], [1, 0], [0, 1], [-1, 0]]; // up, right, down, left
                const [dx, dy] = moves[action % 4];
                
                const newX = Math.max(0, Math.min(this.size - 1, this.agentPos[0] + dx));
                const newY = Math.max(0, Math.min(this.size - 1, this.agentPos[1] + dy));
                
                this.agentPos = [newX, newY];
            }

            render() {
                let html = '';
                for (let y = 0; y < this.size; y++) {
                    for (let x = 0; x < this.size; x++) {
                        let cellClass = 'empty';
                        let content = '';
                        
                        if (x === this.agentPos[0] && y === this.agentPos[1]) {
                            cellClass = 'agent';
                            content = 'A';
                        } else if (x === this.goalPos[0] && y === this.goalPos[1]) {
                            cellClass = 'goal';
                            content = 'G';
                        } else if (this.walls.some(([wx, wy]) => wx === x && wy === y)) {
                            cellClass = 'wall';
                            content = '#';
                        }
                        
                        html += `<div class="grid-cell ${cellClass}">${content}</div>`;
                    }
                    html += '<br>';
                }
                return html;
            }

            getStateSize() { return this.size * this.size + 4; }
        }

        class SortingEnvironment {
            getStateSize() { return 11; }
        }

        class DQNAgent {
            constructor(stateSize, actionSize) {
                this.stateSize = stateSize;
                this.actionSize = actionSize;
            }
        }

        // Global AI instance
        const aiSystem = new AILearningSystem();

        // UI Functions
        function scrollToDemo() {
            document.getElementById('demo').scrollIntoView({ behavior: 'smooth' });
        }

        async function startTraining() {
            if (aiSystem.isTraining) return;
            
            const taskType = document.getElementById('task-type').value;
            const episodes = parseInt(document.getElementById('episodes').value);
            const gridSize = parseInt(document.getElementById('grid-size').value);
            
            const startBtn = document.getElementById('start-training');
            startBtn.disabled = true;
            startBtn.innerHTML = '<span class="loading"></span> Training...';
            
            aiSystem.createTask(taskType, { size: gridSize });
            await aiSystem.train(episodes);
            
            startBtn.disabled = false;
            startBtn.innerHTML = 'üöÄ Start Training AI';
        }

        async function testAI() {
            const testBtn = document.getElementById('test-ai');
            testBtn.disabled = true;
            testBtn.innerHTML = '<span class="loading"></span> Testing...';
            
            await aiSystem.testAI();
            
            testBtn.disabled = false;
            testBtn.innerHTML = 'üéØ Test Trained AI';
        }

        function resetDemo() {
            aiSystem.reset();
            document.getElementById('start-training').disabled = false;
            document.getElementById('start-training').innerHTML = 'üöÄ Start Training AI';
        }

        function downloadCode() {
            // Create downloadable file with the complete Python code
            const pythonCode = `# AI Learning System - Complete Source Code
# Deep Q-Network Reinforcement Learning Implementation

import numpy as np
import json
import pickle
from typing import List, Tuple, Optional, Dict, Any, Callable
import random
from abc import ABC, abstractmethod
import time

class Memory:
    """Experience replay memory for storing and learning from past actions"""
    
    def __init__(self, capacity: int = 10000):
        self.capacity = capacity
        self.memory = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        """Store an experience"""
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        
        self.memory[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int):
        """Sample random experiences for training"""
        if len(self.memory) < batch_size:
            return random.sample(self.memory, len(self.memory))
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

# ... [Include your complete code from paste.txt here] ...

if __name__ == "__main__":
    print("ü§ñ Action-Learning AI System")
    print("=" * 40)
    
    # Create the learning AI
    ai = LearningAI()
    
    # Add different tasks
    print("\\nüìã Adding Tasks...")
    
    # Task 1: Navigate a grid world
    grid_env = GridWorldEnvironment(width=4, height=4)
    ai.add_task("navigation", grid_env)
    
    # Task 2: Learn to sort numbers
    sorting_env = TaskEnvironment(task_type="sorting")
    ai.add_task("sorting", sorting_env)
    
    # Train and demonstrate
    ai.learn_task("navigation", episodes=500)
    ai.demonstrate_task("navigation", episodes=2)
`;

            const link = document.createElement('a');
            link.href = 'data:text/plain;charset=utf-8,' + encodeURIComponent(pythonCode);
            link.download = 'ai_learning_system.py';
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
            
            aiSystem.log('üì• Downloaded complete Python source code!');
        }

        // Task type change handler
        document.getElementById('task-type').addEventListener('change', function() {
            const navConfig = document.getElementById('nav-config');
            if (this.value === 'navigation') {
                navConfig.style.display = 'block';
            } else {
                navConfig.style.display = 'none';
            }
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth' });
                }
            });
        });

        // Initialize demo on page load
        window.addEventListener('load', function() {
            aiSystem.log('üéâ AI Learning System loaded and ready!');
            aiSystem.log('üí° Select a task and start training to see the AI learn in real-time.');
        });
